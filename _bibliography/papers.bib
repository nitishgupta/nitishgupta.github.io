---
---

@phdthesis{gupta2021thesis,
  author={Gupta, Nitish},
  year={2021},
  title={Learning to Compositionally Reason over Natural Language},
  journal={ProQuest Dissertations and Theses},
  abstract={The human ability to understand the world in terms of reusable ``building blocks'' allows us to generalize in near-infinite ways. Developing language understanding systems that can compositionally reason in a similar manner is crucial to achieve human-like capabilities. Designing such systems presents key challenges in the architectural design of machine learning models and the learning paradigm used to train them. This dissertation addresses aspects of both of these challenges by exploring compositional structured models that can be trained using end-task supervision. We believe that solving complex problems in a generalizable manner requires decomposition into sub-tasks, which in turn are solved using reasoning capabilities that can be reused in novel contexts. Motivated by this idea, we develop a neuro-symbolic model with a modular architecture for language understanding and focus on answering questions requiring multi-step reasoning against natural language text. We design an inventory of freely-composable, learnable neural modules for performing various atomic language understanding and symbolic reasoning tasks in a differentiable manner. The question guides how these modules are dynamically composed to yield an end-to-end differentiable model that performs compositional reasoning and can be trained using end-task supervision. However, we show that when trained using such supervision, having a compositional model structure is not sufficient to induce the intended problem decomposition in terms of the modules; Lack of supervision for the sub-tasks leads to modules that do not freely compose in novel ways, hurting generalization. To address this, we develop a new training paradigm that leverages paired examples---instances that share sub-tasks---to provide an additional training signal to that provided by individual examples. We show that this paradigm induces the intended compositional reasoning and leads to improved in- and out-of-distribution generalization.},
  pdf={/assets/pdf/nitish-gupta-thesis.pdf},
}

@inproceedings{gupta2021sp,
  title={Enforcing Consistency in Weakly Supervised Semantic Parsing},
  author={Gupta, Nitish and Singh, Sameer and Gardner, Matt},
  booktitle={Proceedings of the Association for Computational Linguistics (ACL)},
  year={2021},
  pdf={/assets/pdf/acl21-cameraready.pdf},
  code={https://github.com/nitishgupta/allennlp-semparse/tree/nlvr-v2/scripts/nlvr_v2},
  abstract={The predominant challenge in weakly supervised semantic parsing is that of spurious programs that evaluate to correct answers for the wrong reasons. Prior work uses elaborate search strategies to mitigate the prevalence of spurious programs; however, they typically consider only one input at a time. In this work we explore the use of consistency between the output programs for related inputs to reduce the impact of spurious programs. We bias the program search (and thus the model’s training signal) towards programs that map the same phrase in related inputs to the same sub-parts in their respective programs. Additionally, we study the importance of designing logical formalisms that facilitate this kind of consistency-based training. We find that a more consis-tent formalism leads to improved model performance even without consistency-based training. When combined together, these two insights lead to a 10% absolute improvement over the best prior result on the Natural Language Visual Reasoning dataset.}
}

@inproceedings{gupta2020neural,
  title={Neural Module Networks for Reasoning over Text},
  author={Gupta, Nitish and Lin, Kevin and Roth, Dan and Singh, Sameer and Gardner, Matt},
  booktitle={International Conference for Learning Representations (ICLR)},
  year={2020},
  arxiv={1912.04971},
  pdf={https://arxiv.org/pdf/1912.04971.pdf},
  code={https://nitishgupta.github.io/nmn-drop},
  abstract={Answering compositional questions that require multiple steps of reasoning against text is challenging, especially when they involve discrete, symbolic operations. Neural module networks (NMNs) learn to parse such questions as executable programs composed of learnable modules, performing well on synthetic visual QA domains. However, we find that it is challenging to learn these models for non-synthetic questions on open-domain text, where a model needs to deal with the diversity of natural language and perform a broader range of reasoning. We extend NMNs by: (a) introducing modules that reason over a paragraph of text, performing symbolic reasoning (such as arithmetic, sorting, counting) over numbers and dates in a probabilistic and differentiable manner; and (b) proposing an unsupervised auxiliary loss to help extract arguments associated with the events in text. Additionally, we show that a limited amount of heuristically-obtained question program and intermediate module output supervision provides sufficient inductive bias for accurate learning. Our proposed model significantly outperforms state-of-the-art models on a subset of the DROP dataset that poses a variety of reasoning challenges that are covered by our modules.}
}

@inproceedings{subramanian2020obtaining,
  title={Obtaining Faithful Interpretations from Compositional Neural Networks},
  author={Subramanian*, Sanjay and Bogin*, Ben and Gupta*, Nitish and Wolfson, Tomer and Singh, Sameer and Berant, Jonathan and Gardner, Matt},
  booktitle={Proceedings of the Association for Computational Linguistics (ACL)},
  year={2020},
  arxiv={2005.00724},
  pdf={https://arxiv.org/pdf/2005.00724.pdf},
  code={https://github.com/allenai/faithful-nmn}
}

@inproceedings{oren2020semparse,
  title={Improving Compositional Generalization in Semantic Parsing},
  author={Inbar Oren and Jonathan Herzig* and Nitish Gupta* and Matt Gardner and Jonathan Berant},
  booktitle={Findings of EMNLP},
  year={2020},
  arxiv={2010.05647},
  pdf={https://arxiv.org/pdf/2010.05647.pdf},
  code={https://github.com/inbaroren/improving-compgen-in-semparse}
}

@inproceedings{gardner2020evaluating,
  title={Evaluating Models’ Local Decision Boundaries via Contrast Sets},
  author={Gardner, Matt and Artzi, Yoav and Basmova, Victoria and Berant, Jonathan and Bogin, Ben and Chen, Sihao and Dasigi, Pradeep and Dua, Dheeru and Elazar, Yanai and Gottumukkala, Ananth and others},
  booktitle={Findings of EMNLP},
  year={2020},
  arxiv={2004.02709},
  pdf={https://arxiv.org/pdf/2004.02709.pdf},
  code={https://github.com/allenai/contrast-sets}
}

@inproceedings{shah2020mcqa,
  title={What do we expect from multiple-choice QA Systems?},
  author={Shah, Krunal and Gupta, Nitish and Roth, Dan},
  booktitle={Findings of EMNLP},
  year={2020},
  arxiv={2011.10647},
  pdf={https://arxiv.org/pdf/2011.10647.pdf},
  code={https://github.com/CogComp/mcqa-expectations}
}


@inproceedings{kodner2020overestimation,
  title={Overestimation of Syntactic Representations in Neural Language Models},
  author={Kodner, Jordan and Gupta, Nitish},
  booktitle={Proceedings of the Association for Computational Linguistics (ACL)},
  year={2020},
  arxiv={2004.05067},
  pdf={https://arxiv.org/pdf/2004.05067.pdf}
}

@inproceedings{mayhew2020robust,
  title={Robust Named Entity Recognition with Truecasing Pretraining},
  author={Mayhew, Stephen and Gupta, Nitish and Roth, Dan},
  booktitle={Association for Advacements in Artificial Intelligence (AAAI)},
  year={2020},
  arxiv={1912.07095},
  pdf={https://arxiv.org/pdf/1912.07095.pdf}
}

@inproceedings{gupta2018neural,
  title={Neural Compositional Denotational Semantics for Question Answering},
  author={Gupta, Nitish and Lewis, Mike},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2018},
  arxiv={1808.09942},
  pdf={https://arxiv.org/pdf/1808.09942.pdf},
}

@inproceedings{upadhyay2018joint,
  title={Joint multilingual supervision for cross-lingual entity linking},
  author={Upadhyay, Shyam and Gupta, Nitish and Roth, Dan},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2018},
  arxiv={1809.07657},
  pdf={https://arxiv.org/pdf/1809.07657.pdf},
  code={https://github.com/shyamupa/xelms}
}


@inproceedings{gupta2017entity,
  title={Entity linking via joint encoding of types, descriptions, and context},
  author={Gupta, Nitish and Singh, Sameer and Roth, Dan},
  booktitle={Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={2681--2690},
  year={2017},
  code={https://nitishgupta.github.io/neural-el/},
  html={https://www.aclweb.org/anthology/D17-1284/},
  pdf={https://www.aclweb.org/anthology/D17-1284.pdf}
}

@inproceedings{shyam2016revisiting,
  title={Revisiting the Evaluation for Cross Document Event Coreference},
  author={Upadhyay, Shyam and Gupta, Nitish and Christodoulopoulos, Christos and Roth, Dan},
  booktitle={Proceedings of the 2016 International Conference on Computational Linguistics (COLING)},
  year={2016},
  html={https://www.aclweb.org/anthology/C16-1183/},
  pdf={https://www.aclweb.org/anthology/C16-1183.pdf}
}


@article{gupta2015collectively,
  title={Collectively embedding multi-relational data for predicting user preferences},
  author={Gupta, Nitish and Singh, Sameer},
  journal={Technical Report},
  year={2015},
  metadata={Won the Yelp Dataset Challenge Round 4},
  arxiv={1504.06165},
  pdf={https://arxiv.org/pdf/1504.06165.pdf}
}
